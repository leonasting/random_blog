
Cloud Composer API needs to be enabled.
You can have separate service account for Cloud composer.

Composer 1: No Autoscaling
Composer 2: Autoscaling 

Name: XXX
Account : Service account

There are three architecture configuration in which cloud composer 2 can be created:
* Public IP 
* Private Ip
* Highly Resilient Private Ip

When create an environment cloud composer allocates resources for two projects. First one is our own google cloud project i.e. **Customer Project** and other is **Tenant Project** which takes over the access control ,security and meta data for the Customer Project.

Please read more from official Page of [Cloud Composer Architecture](https://cloud.google.com/composer/docs/composer-2/environment-architecture)


Things that are created when we start a cloud composer environment(In no specific order):-
* Airflow Database: CLoud SQL instance - Online DB
	* Cloud SQL Instance  which is like the **Airflow Database** present in Tennant Project.
* Cloud SQL Storage: Cloud Storage
	* It stores the content of cloud SQL in tenant Project.
* Cloud SQL Proxy connects all the airflow components to the airflow database in tenant project.
	* Single access point from customer- tenant
* Composer Agent-  Deals with changes in environment like create/del environment and runs as job in your environment cluster.
* AIrflow monitoring - Monitoring the ussage and sharing with cloud monitoring
* Airflow Init DB -Cloud SQL instance and initial database schema. Airflow InitDB runs as a [Job](https://kubernetes.io/docs/concepts/workloads/controllers/job/) in your environment's cluster.
* - _FluentD_. Collects logs from all environment components and uploads the logs to Cloud Logging. Runs as a [DaemonSet](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/) in your environment's cluster.
* - _Cloud Storage FUSE_. [Mounts your environment's bucket as a file system](https://cloud.google.com/storage/docs/gcs-fuse) on Airflow workers, schedulers, and web server, so that these components can access the data from the bucket. Runs as a [DaemonSet](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/) in your environment's cluster.
	* Temporary file sturage  for instances. It can have configuration 
* GKE cluster - Contains - Authoring, Scheduling 


These services are deployed as Kubernetes cluster on Google Cloud.

Kubernetes Engine-> Workload
https://www.youtube.com/watch?v=ukM1zSUiEYI&list=PLLrA_pU9-Gz22Zml5mxcszG4A9ecqWtd4&index=5
